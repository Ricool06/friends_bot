{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is intended to:\n",
    "- Ingest the entire collected works of the TV show \"Friends\"\n",
    "- Preprocess the data by extracting text and concatenating into a single document\n",
    "- Train a Natural Language Processing model to generate similar works\n",
    "- Evaluate the model\n",
    "- Save the model so it may be used by a Twitter bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://fangj.github.io/friends/'\n",
    "links = List[str]\n",
    "\n",
    "with requests.get(base_url) as response:\n",
    "    html = BeautifulSoup(response.text)\n",
    "    links = [a['href'] for a in html.find_all('a')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_script(i: int, path: str) -> str:\n",
    "    with requests.get(base_url + path) as page_res:\n",
    "        page_html = BeautifulSoup(page_res.text)\n",
    "    \n",
    "    try:\n",
    "        first_scene_annotation = page_html.find(text=re.compile('Scene:'))\n",
    "        after = first_scene_annotation.parent.find_next_siblings()\n",
    "\n",
    "        return '\\n'.join([first_scene_annotation] + [el.text for el in after])\n",
    "    except:\n",
    "        raise Exception('Loop failed on iteration: %d' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_friends_script = [extract_script(i, link) for i, link in enumerate(links) if i not in [26, 34]]\n",
    "# Episode 26 & 34 don't follow the pattern of transcription seen in other episode scripts.\n",
    "# They lack the first '[Scene: ...]' stage direction\n",
    "# Recommend a PR to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_friends_script = '\\n'.join(entire_friends_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\\nMonica: There's nothing to tell! He's just some guy\\nI work with!\\nJoey: C'mon, you're going out with the guy! There's\\ngotta be something wrong with him!\\nChandler: All right Joey, b\""
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "entire_friends_script[:248]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest is now done.\n",
    "The `entire_friends_script` variable holds the concatenated scripts for all the Friends episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(entire_friends_script))\n",
    "char_to_ind = {u:i for i, u in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_scripts = np.array([char_to_ind[c] for c in entire_friends_script])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = Dataset.from_tensor_slices(encoded_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "desired_sequence_length = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(desired_sequence_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_target_pairs(sequence: str) -> (str, str):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('This is just some test text lo', 'his is just some test text lol')"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "create_input_target_pairs('This is just some test text lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_input_target_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "\n",
    "shuffled_dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_size=batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset created\n",
    "Pairs of sequences shifted by 1 character have been shuffled into a dataset\n",
    "\n",
    "'Hello, I am Ricoo' -> 'ello, I am Ricool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dimension = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dimension,\n",
    "    batch_input_shape=[batch_size, None]))\n",
    "\n",
    "model.add(GRU(\n",
    "    1026,\n",
    "    return_sequences=True,\n",
    "    stateful=True,\n",
    "    recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(vocab_size))\n",
    "\n",
    "model.compile(optimizer='adam', loss=sparse_cat_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (128, None, 64)           6976      \n_________________________________________________________________\ngru (GRU)                    (128, None, 1026)         3361176   \n_________________________________________________________________\ndense (Dense)                (128, None, 109)          111943    \n=================================================================\nTotal params: 3,480,095\nTrainable params: 3,480,095\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model built\n",
    "The next bit is just to confirm the model shape and training is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(128, 140, 109)\n"
    }
   ],
   "source": [
    "for input_example, target_example in shuffled_dataset.take(1):\n",
    "    example_preds = model(input_example)\n",
    "\n",
    "    print(example_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.random import categorical\n",
    "from tensorflow import squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_preds_categories = categorical(example_preds[0], num_samples=1)\n",
    "example_preds_categories = squeeze(example_preds_categories, axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'OUH�aJdéy&dÉOJm[q?f4’W:)\\xa0h9,\";+”:EpBÉ—égté0\\xa0m}0p*É<Q/%12z\"oD^”_g=—d,pTq\\r\\r<76r[vqPP-…K“!cçL(|UC1rJB6:.5{J%Qd—!f14C/]52\\r”M6jL W,!m2”LVt,+ÉMX=”'"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "''.join(ind_to_char[example_preds_categories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape is correct\n",
    "Now to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/120\n170/170 [==============================] - 36s 209ms/step - loss: 2.0262\nEpoch 2/120\n170/170 [==============================] - 35s 209ms/step - loss: 1.6941\nEpoch 3/120\n170/170 [==============================] - 36s 209ms/step - loss: 1.4991\nEpoch 4/120\n170/170 [==============================] - 35s 207ms/step - loss: 1.3824\nEpoch 5/120\n170/170 [==============================] - 35s 208ms/step - loss: 1.3088\nEpoch 6/120\n170/170 [==============================] - 35s 208ms/step - loss: 1.2572\nEpoch 7/120\n170/170 [==============================] - 35s 207ms/step - loss: 1.2183\nEpoch 8/120\n170/170 [==============================] - 35s 206ms/step - loss: 1.1878\nEpoch 9/120\n170/170 [==============================] - 35s 207ms/step - loss: 1.1622\nEpoch 10/120\n170/170 [==============================] - 35s 208ms/step - loss: 1.1392\nEpoch 11/120\n170/170 [==============================] - 36s 209ms/step - loss: 1.1193\nEpoch 12/120\n170/170 [==============================] - 36s 210ms/step - loss: 1.1009\nEpoch 13/120\n170/170 [==============================] - 35s 209ms/step - loss: 1.0840\nEpoch 14/120\n170/170 [==============================] - 36s 209ms/step - loss: 1.0677\nEpoch 15/120\n170/170 [==============================] - 36s 209ms/step - loss: 1.0517\nEpoch 16/120\n170/170 [==============================] - 36s 210ms/step - loss: 1.0366\nEpoch 17/120\n170/170 [==============================] - 35s 209ms/step - loss: 1.0217\nEpoch 18/120\n170/170 [==============================] - 35s 209ms/step - loss: 1.0073\nEpoch 19/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.9920\nEpoch 20/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.9780\nEpoch 21/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.9635\nEpoch 22/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.9503\nEpoch 23/120\n170/170 [==============================] - 35s 204ms/step - loss: 0.9359\nEpoch 24/120\n170/170 [==============================] - ETA: 0s - loss: 0.923170/170 [==============================] - 35s 207ms/step - loss: 0.9235\nEpoch 25/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.9101\nEpoch 26/120\n170/170 [==============================] - 36s 211ms/step - loss: 0.8977\nEpoch 27/120\n170/170 [==============================] - 36s 211ms/step - loss: 0.8869\nEpoch 28/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.8749\nEpoch 29/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.8645\nEpoch 30/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.8542\nEpoch 31/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.8443\nEpoch 32/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.8362\nEpoch 33/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.8284\nEpoch 34/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.8213\nEpoch 35/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.8140\nEpoch 36/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.8081\nEpoch 37/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.8024\nEpoch 38/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.7975\nEpoch 39/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.7915\nEpoch 40/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7881\nEpoch 41/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7839\nEpoch 42/120\n170/170 [==============================] - ETA: 0s - loss: 0.780170/170 [==============================] - 36s 209ms/step - loss: 0.7800\nEpoch 43/120\n170/170 [==============================] - 36s 213ms/step - loss: 0.7766\nEpoch 44/120\n170/170 [==============================] - 36s 210ms/step - loss: 0.7724\nEpoch 45/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7690\nEpoch 46/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7674\nEpoch 47/120\n170/170 [==============================] - 36s 211ms/step - loss: 0.7657\nEpoch 48/120\n170/170 [==============================] - 36s 210ms/step - loss: 0.7623\nEpoch 49/120\n170/170 [==============================] - 36s 211ms/step - loss: 0.7604\nEpoch 50/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7575\nEpoch 51/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7560\nEpoch 52/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7556\nEpoch 53/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.7536\nEpoch 54/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7521\nEpoch 55/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7499\nEpoch 56/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7495\nEpoch 57/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.7486\nEpoch 58/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7456\nEpoch 59/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7460\nEpoch 60/120\n170/170 [==============================] - 35s 205ms/step - loss: 0.7451\nEpoch 61/120\n170/170 [==============================] - 35s 203ms/step - loss: 0.7443\nEpoch 62/120\n170/170 [==============================] - 36s 210ms/step - loss: 0.7429\nEpoch 63/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.7430\nEpoch 64/120\n170/170 [==============================] - 34s 200ms/step - loss: 0.7424\nEpoch 65/120\n170/170 [==============================] - 34s 201ms/step - loss: 0.7418\nEpoch 66/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7410\nEpoch 67/120\n170/170 [==============================] - 34s 200ms/step - loss: 0.7404\nEpoch 68/120\n170/170 [==============================] - 34s 199ms/step - loss: 0.7400\nEpoch 69/120\n170/170 [==============================] - 35s 203ms/step - loss: 0.7405\nEpoch 70/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.7408\nEpoch 71/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7389\nEpoch 72/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.7390\nEpoch 73/120\n170/170 [==============================] - 34s 201ms/step - loss: 0.7384\nEpoch 74/120\n170/170 [==============================] - 34s 198ms/step - loss: 0.7421\nEpoch 75/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.7402\nEpoch 76/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7387\nEpoch 77/120\n170/170 [==============================] - 35s 205ms/step - loss: 0.7394\nEpoch 78/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7396\nEpoch 79/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7416\nEpoch 80/120\n170/170 [==============================] - 35s 204ms/step - loss: 0.7422\nEpoch 81/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7435\nEpoch 82/120\n170/170 [==============================] - 35s 205ms/step - loss: 0.7416\nEpoch 83/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7401\nEpoch 84/120\n170/170 [==============================] - 35s 204ms/step - loss: 0.7416\nEpoch 85/120\n170/170 [==============================] - 34s 203ms/step - loss: 0.7406\nEpoch 86/120\n170/170 [==============================] - 34s 201ms/step - loss: 0.7413\nEpoch 87/120\n170/170 [==============================] - 34s 199ms/step - loss: 0.7413\nEpoch 88/120\n170/170 [==============================] - 34s 198ms/step - loss: 0.7421\nEpoch 89/120\n170/170 [==============================] - 34s 202ms/step - loss: 0.7422\nEpoch 90/120\n170/170 [==============================] - 34s 199ms/step - loss: 0.7423\nEpoch 91/120\n170/170 [==============================] - 33s 197ms/step - loss: 0.7435\nEpoch 92/120\n170/170 [==============================] - 33s 196ms/step - loss: 0.7450\nEpoch 93/120\n170/170 [==============================] - 34s 198ms/step - loss: 0.7469\nEpoch 94/120\n170/170 [==============================] - 33s 196ms/step - loss: 0.7472\nEpoch 95/120\n170/170 [==============================] - 33s 197ms/step - loss: 0.7473\nEpoch 96/120\n170/170 [==============================] - 35s 203ms/step - loss: 0.7479\nEpoch 97/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7499\nEpoch 98/120\n170/170 [==============================] - 35s 208ms/step - loss: 0.7575\nEpoch 99/120\n170/170 [==============================] - 35s 206ms/step - loss: 0.7535\nEpoch 100/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.7495\nEpoch 101/120\n170/170 [==============================] - 35s 209ms/step - loss: 0.7522\nEpoch 102/120\n170/170 [==============================] - 35s 204ms/step - loss: 0.7541\nEpoch 103/120\n170/170 [==============================] - 36s 210ms/step - loss: 0.7525\nEpoch 104/120\n170/170 [==============================] - 35s 207ms/step - loss: 0.7549\nEpoch 105/120\n170/170 [==============================] - 35s 203ms/step - loss: 0.7539\nEpoch 106/120\n170/170 [==============================] - 34s 202ms/step - loss: 0.7566\nEpoch 107/120\n170/170 [==============================] - 34s 200ms/step - loss: 0.7563\nEpoch 108/120\n170/170 [==============================] - 33s 197ms/step - loss: 0.7574\nEpoch 109/120\n170/170 [==============================] - 34s 197ms/step - loss: 0.7590\nEpoch 110/120\n170/170 [==============================] - 33s 196ms/step - loss: 0.7652\nEpoch 111/120\n170/170 [==============================] - 33s 195ms/step - loss: 0.7650\nEpoch 112/120\n170/170 [==============================] - 34s 198ms/step - loss: 0.7678\nEpoch 113/120\n170/170 [==============================] - 34s 199ms/step - loss: 0.7688\nEpoch 114/120\n170/170 [==============================] - 34s 199ms/step - loss: 0.7719\nEpoch 115/120\n170/170 [==============================] - 34s 199ms/step - loss: 0.7690\nEpoch 116/120\n170/170 [==============================] - 36s 209ms/step - loss: 0.7723\nEpoch 117/120\n170/170 [==============================] - 34s 201ms/step - loss: 0.7741\nEpoch 118/120\n170/170 [==============================] - 34s 201ms/step - loss: 0.7857\nEpoch 119/120\n170/170 [==============================] - 35s 205ms/step - loss: 0.7975\nEpoch 120/120\n170/170 [==============================] - 35s 204ms/step - loss: 0.7824\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fcb5c14dfa0>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "model.fit(shuffled_dataset, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../func/resources/friends_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import TensorShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 64)             6976      \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1026)           3361176   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 109)            111943    \n=================================================================\nTotal params: 3,480,095\nTrainable params: 3,480,095\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "practice_model = Sequential()\n",
    "\n",
    "practice_model.add(Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dimension,\n",
    "    batch_input_shape=[1, None]))\n",
    "\n",
    "practice_model.add(GRU(\n",
    "    1026,\n",
    "    return_sequences=True,\n",
    "    stateful=True,\n",
    "    recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "practice_model.add(Dense(vocab_size))\n",
    "\n",
    "practice_model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "\n",
    "practice_model.load_weights('../../func/resources/friends_model2.h5')\n",
    "\n",
    "practice_model.build(TensorShape([1, None]))\n",
    "\n",
    "practice_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import expand_dims\n",
    "\n",
    "def generate_text(this_model: Sequential, start_seed: str, num_chars=100, temp=1.0) -> str:\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    " \n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  this_model.reset_states()\n",
    "\n",
    "  for i in range(num_chars):\n",
    "\n",
    "      # Generate Predictions\n",
    "      predictions = this_model(input_eval)\n",
    "\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = squeeze(predictions, 0)\n",
    "\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = expand_dims([predicted_id], 0)\n",
    "\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Monica: Okay. (Starting to use us open it and picks up.)\nRoss: Hey, Mr. Geller!\nPhoebe: That was your miertigion? Oh man! I- What Phoebe make looks disgusted by her wine?\nRoss: No! No, no! No if you’re cheating on you, one marriage\nmuseum is gene!\nRoss: Please, don’t freak out us and Ma\n"
    }
   ],
   "source": [
    "print(generate_text(practice_model, 'Monica:', num_chars=280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../func/resources/vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_model.save('../../func/resources/friends_practice_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('notebooks-4YGLAiTd-py3.8': venv)",
   "language": "python",
   "name": "python_defaultSpec_1596307487537"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}